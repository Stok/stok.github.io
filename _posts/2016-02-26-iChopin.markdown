---
layout: post
title:  "Training a recurrent neural network (rnn) to compose music"
date:   2016-02-26 14:47 +0100
categories: Projects
---

Karpathy implemented an [rnn that learns text]. His readme file describes his work as an implementation of a
"**multi-layer Recurrent Neural Network** (RNN, LSTM, and GRU) for training/sampling from character-level language models. 
The model learns to predict the probability of the next character in a sequence. 
In other words, the input is a single text file and the model learns to generate text like it."
In his tutorial, he uses Shakespeare's work as the training corpus and shows that the computer is able to generate dialog that, 
although isn't Shakespeare, has really captured the essence of his plays.

A friend of mine trained Karpathy's rnn to generate fake staff bulletins with absolutely hilarious results. 
It was so funny that had to get in on this. My first attempt was to teach it the French "code civil" 
(a huge unreadable compilation of French laws), which I'll describe elsewhere.
Here I'm trying something else: if I could convert music into character form, 
I might be able to teach my computer to compose music! A composing-rnn, broadcasting live out into the wilderness of the internet.

I mostly treat the rnn code as a black box. But Karpathy's code (as well as his "Usage" guide) is simple enough to be able to get it up and running quickly.
Once you've done his Shakespeare example, all you really have to do is to replace the input text with whatever you want your rnn to learn.
 
This meant that for my music project, the main task was finding the right format to use. 
You need a format that efficiently highlights the 'patterns' that are found in music. Notes, keys, chord progressions...
in text form. And it has to be encoded in a way that the rnn can "remember" these things despite a finite sequence length.
The rnn only uses a finite number of characters to look for correlations, so writing the information out once at the beginning of a piece doesn't work very well.
With this in mind, I started trying to develop a simple encoding scheme that the rnn could learn. 
After many rather dubious results (which I'll maybe describe elsewhere), 
I've converged on a system that seems to produce something that might be called music.

Note: A lot of decisions were made based on "that was interesting!?". It may not be to everyone's taste.
I decided to work with classical piano music. I figured it would be rich enough to pose a real challenge for the rnn, 
and having a single instrument must make the encoding easier.
So, Chopin. (I actually started with Bach, but personal taste made me switch despite the added complexity).

I found [a website] that keeps classical piano pieces on MIDI. I downloaded everything they had for Chopin, and started 
writing my converters using Mathematica.

The result is 2 notebooks:
 
- One that converts a bunch of MIDI files (.mid) into an rnn-"input.txt" corpus file.

- Another to convert the output of the rnn back to a midi file.

The format is basically TSV:

time-since-last-note \t duration \t note \t octave \n

If a chord is played, the row gets longer:

time-since-last-note \t duration1 \t note1 \t octave1 \t duration2 \t note2 \t octave2... \n

Here's an example of some output:

![example](/photos/outputtxtexample.png)

I found this format works "best". And this is an example of a composition.

<script type='text/javascript' src='http://midijs.net/lib/midi.js'>
</script>

<button type="button" onclick="MIDIjs.play('../../../../midi/output2.mid');"><b>Play Sample</b></button>
<button type="button" onclick="MIDIjs.stop();"><b>Stop</b></button>

This represents 10000 chars generated by an rnn at a temperature of 0.7. 
Training parameters were: sequence length of 75, rnn_size of 512, 2 num_layers and a dropout of 0.5.

OK, it's nowhere near Chopin. But it's doing complicated stuff: 
changes in tempo and an occasional chord progression that isn't too dissonant. OK, its still pretty dissonnant at times.
But I kept telling the rnn to generate longer and longer pieces just to see what it would do next.
I figure that's a pretty good start.

A few examples of things that worked less well:
 
- Mathematica returns start and end times of notes when you Import MIDI files. But if you use those directly, you seem to end up with a column
comprising the average start/end time of a note in a piece. 
Result: "Silence for a minute", chaotic explosion of 100 notes played in a 10 second interval, then silence for another minute.

- Assembling chords into a single row is good. Without it, the result feels a lot more "scattered". 
It feels like the rnn fails to notice that some notes tend to appear together. 

Remember that these are all made using the same pieces as the input corpus. 
It's really "how I'm presenting the data to the rnn" which seems to be making the difference. 

If you want to use these converters, they can be found [here]. Put the notebook in a folder with your .mid files, and run.
It'll spit out a "input.txt" file that can be used in Karpathy's rnn. For listening to the result, save the rnn's output to a file 
(by adding " > output.txt" to the command line for sampling, say). 
Make sure you delete the initial gibberish in the text file, and keep only the table of numbers.
Run the other converter to generate a playable midi file.

I'm still trying out new formats, and if anyone has any ideas, please let me know!
I'll stop here for this post. Next step: getting this thing online!


[rnn that learns text]: https://github.com/karpathy/char-rnn
[a website]: http://www.piano-midi.de/chopin.htm
[here]: https://github.com/Stok/Chopino
